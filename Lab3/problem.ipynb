{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "dlf4P1apdf-w"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import argparse\n",
    "import time\n",
    "from collections import Counter\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "\n",
    "# Define a class to store a single sentiment example\n",
    "class SentimentExample:\n",
    "    def __init__(self, words, label):\n",
    "        self.words = words\n",
    "        self.label = label\n",
    "\n",
    "    def __repr__(self):\n",
    "        return repr(self.words) + \"; label=\" + repr(self.label)\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "\n",
    "# Reads sentiment examples in the format [0 or 1]<TAB>[raw sentence]; tokenizes and cleans the sentences.\n",
    "def read_sentiment_examples(infile):\n",
    "    f = open(infile, encoding='iso8859')\n",
    "    exs = []\n",
    "    for line in f:\n",
    "            fields = line.strip().split(\" \")\n",
    "            label = 0 if \"0\" in fields[0] else 1\n",
    "            exs.append(SentimentExample(fields[1:], label))\n",
    "    f.close()\n",
    "    return exs\n",
    "\n",
    "\n",
    "# Bijection between objects and integers starting at 0. Useful for mapping\n",
    "# labels, features, etc. into coordinates of a vector space.\n",
    "# This class creates a mapping between objects (here words) and unique indices\n",
    "# For example: apple->1, banana->2, and so on\n",
    "class Indexer(object):\n",
    "    def __init__(self):\n",
    "        self.objs_to_ints = {}\n",
    "        self.ints_to_objs = {}\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str([str(self.get_object(i)) for i in range(0, len(self))])\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.__repr__()\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.objs_to_ints)\n",
    "\n",
    "    # Returns the object corresponding to the particular index\n",
    "    def get_object(self, index):\n",
    "        if (index not in self.ints_to_objs):\n",
    "            return None\n",
    "        else:\n",
    "            return self.ints_to_objs[index]\n",
    "\n",
    "    def contains(self, object):\n",
    "        return self.index_of(object) != -1\n",
    "\n",
    "    # Returns -1 if the object isn't present, index otherwise\n",
    "    def index_of(self, object):\n",
    "        if (object not in self.objs_to_ints):\n",
    "            return -1\n",
    "        else:\n",
    "            return self.objs_to_ints[object]\n",
    "\n",
    "    # Adds the object to the index if it isn't present, always returns a nonnegative index\n",
    "    def add_and_get_index(self, object, add=True):\n",
    "        if not add:\n",
    "            return self.index_of(object)\n",
    "        if (object not in self.objs_to_ints):\n",
    "            new_idx = len(self.objs_to_ints)\n",
    "            self.objs_to_ints[object] = new_idx\n",
    "            self.ints_to_objs[new_idx] = object\n",
    "        return self.objs_to_ints[object]\n",
    "\n",
    "\n",
    "# Feature extraction base type. Takes an example and returns an indexed list of features.\n",
    "class FeatureExtractor(object):\n",
    "    # Extract features. Includes a flag add_to_indexer to control whether the indexer should be expanded.\n",
    "    # At test time, any unseen features should be discarded, but at train time, we probably want to keep growing it.\n",
    "    def extract_features(self, ex, add_to_indexer):\n",
    "        raise Exception(\"Don't call me, call my subclasses\")\n",
    "\n",
    "\n",
    "# Extracts unigram bag-of-words features from a sentence. It's up to you to decide how you want to handle counts\n",
    "class UnigramFeatureExtractor(FeatureExtractor):\n",
    "    def __init__(self, indexer: Indexer):\n",
    "        self.indexer = indexer\n",
    "\n",
    "    def extract_features(self, ex, add_to_indexer=False):\n",
    "        features = Counter()\n",
    "        for w in ex.words:\n",
    "            feat_idx = self.indexer.add_and_get_index(w) \\\n",
    "                if add_to_indexer else self.indexer.index_of(w)\n",
    "            if feat_idx != -1:\n",
    "                features[feat_idx] += 1.0\n",
    "        return features\n",
    "\n",
    "\n",
    "# Bigram feature extractor analogous to the unigram one.\n",
    "class BigramFeatureExtractor(FeatureExtractor):\n",
    "    def __init__(self, indexer: Indexer):\n",
    "        self.indexer = indexer\n",
    "\n",
    "    def extract_features(self, ex, add_to_indexer=False):\n",
    "        features = Counter()\n",
    "        for i in range(len(ex.words) - 1):\n",
    "            w = ex.words[i] + \"||\" + ex.words[i + 1]\n",
    "            feat_idx = self.indexer.add_and_get_index(w) \\\n",
    "                if add_to_indexer else self.indexer.index_of(w)\n",
    "            if feat_idx != -1:\n",
    "                features[feat_idx] += 1.0\n",
    "        return features\n",
    "\n",
    "\n",
    "# Sentiment classifier base type\n",
    "class SentimentClassifier(object):\n",
    "    # Makes a prediction for the given\n",
    "    def predict(self, ex: SentimentExample):\n",
    "        raise Exception(\"Don't call me, call my subclasses\")\n",
    "\n",
    "\n",
    "class LogisticRegressionClassifier(SentimentClassifier):\n",
    "    def __init__(self, feat_extractor: FeatureExtractor, train_examples, num_iters=50, reg_lambda=0.0, learning_rate=0.1):\n",
    "        self.weights = None  # We will initialize it later based on feature size\n",
    "        self.bias = 0.0\n",
    "        self.train(feat_extractor, train_examples, num_iters, reg_lambda, learning_rate)\n",
    "        \n",
    "        \n",
    "    def train(self, feat_extractor: FeatureExtractor, train_examples, num_iters=50, reg_lambda=0.0, learning_rate=0.1):\n",
    "        # Extract features for training examples\n",
    "        features = []\n",
    "        labels = []\n",
    "        for train_ex in train_examples:\n",
    "            labels.append(train_ex.label)\n",
    "            features.append(feat_extractor.extract_features(ex=train_ex, add_to_indexer=True))\n",
    "        \n",
    "        # Convert features to matrix format\n",
    "        num_features = len(feat_extractor.indexer.ints_to_objs)\n",
    "        num_train_examples = len(features)\n",
    "        feature_matrix = np.zeros((num_train_examples, num_features))\n",
    "        for i, feature in enumerate(features):\n",
    "            for index in feature:\n",
    "                feature_matrix[i][index] = feature[index]\n",
    "        \n",
    "        # Initialize weights and bias\n",
    "        self.weights = np.zeros(num_features)\n",
    "        self.bias = 0.0\n",
    "        \n",
    "        # Gradient descent optimization\n",
    "        for _ in range(num_iters):\n",
    "            # Compute predictions\n",
    "            predictions = np.dot(feature_matrix, self.weights) + self.bias\n",
    "            \n",
    "            # Compute gradients\n",
    "            gradient_weights = np.dot(feature_matrix.T, (predictions - labels)) / num_train_examples\n",
    "            gradient_bias = np.sum(predictions - labels) / num_train_examples\n",
    "            \n",
    "            # Update weights and bias\n",
    "            self.weights -= learning_rate * (gradient_weights + reg_lambda * self.weights)\n",
    "            self.bias -= learning_rate * gradient_bias\n",
    "\n",
    "        self.feat_extractor = feat_extractor\n",
    "    \n",
    "    def predict(self, ex):\n",
    "        # Extract features for test example\n",
    "        features = self.feat_extractor.extract_features(ex=ex, add_to_indexer=False)\n",
    "        \n",
    "        # Convert features to vector format\n",
    "        num_features = len(self.feat_extractor.indexer.ints_to_objs)\n",
    "        feature_vector = np.zeros(num_features)\n",
    "        for index in features:\n",
    "            feature_vector[index] = features[index]\n",
    "        \n",
    "        # Compute prediction\n",
    "        prediction = np.dot(feature_vector, self.weights) + self.bias\n",
    "        \n",
    "        # Threshold prediction\n",
    "        if prediction >= 0.5:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xnCGie6NGB4X"
   },
   "outputs": [],
   "source": [
    "# Train a logsitic regression model on the given training examples using the given FeatureExtractor\n",
    "def train_lr(train_exs: List[SentimentExample], feat_extractor: FeatureExtractor, reg_lambda) -> LogisticRegressionClassifier:\n",
    "    # Initialize the logistic regression classifier with the provided feature extractor,\n",
    "    # training examples, and other hyperparameters like regularization lambda.\n",
    "    lr_classifier = LogisticRegressionClassifier(feat_extractor, train_exs, num_iters=440, reg_lambda=reg_lambda, learning_rate=0.1)\n",
    "    return lr_classifier\n",
    "\n",
    "\n",
    "# Main entry point for your modifications. Trains and returns one of several models depending on the options passed\n",
    "def train_model(feature_type, model_type, train_exs, reg_lambda=0.0):\n",
    "    # Initialize feature extractor\n",
    "    if feature_type == \"unigram\":\n",
    "        # Add additional preprocessing code here\n",
    "        feat_extractor = UnigramFeatureExtractor(Indexer())\n",
    "    elif feature_type == \"bigram\":\n",
    "        # Add additional preprocessing code here\n",
    "        feat_extractor = BigramFeatureExtractor(Indexer())\n",
    "    else:\n",
    "        raise Exception(\"Pass unigram or bigram\")\n",
    "\n",
    "    if model_type == \"LogisticRegression\":\n",
    "        model = train_lr(train_exs, feat_extractor, reg_lambda=reg_lambda)\n",
    "    else:\n",
    "        raise Exception(\"Pass LogisticRegression\")\n",
    "    return model\n",
    "\n",
    "\n",
    "# Evaluates a given classifier on the given examples\n",
    "def evaluate(classifier, exs):\n",
    "    return print_evaluation([ex.label for ex in exs], [classifier.predict(ex) for ex in exs])\n",
    "\n",
    "\n",
    "# Prints accuracy comparing golds and predictions, each of which is a sequence of 0/1 labels.\n",
    "def print_evaluation(golds, predictions):\n",
    "    num_correct = 0\n",
    "    num_pos_correct = 0\n",
    "    num_pred = 0\n",
    "    num_gold = 0\n",
    "    num_total = 0\n",
    "\n",
    "    if len(golds) != len(predictions):\n",
    "        raise Exception(\"Mismatched gold/pred lengths: %i / %i\" %\n",
    "                        (len(golds), len(predictions)))\n",
    "\n",
    "    for idx in range(0, len(golds)):\n",
    "        gold = golds[idx]\n",
    "        prediction = predictions[idx]\n",
    "        if prediction == gold:\n",
    "            num_correct += 1\n",
    "        if prediction == 1:\n",
    "            num_pred += 1\n",
    "        if gold == 1:\n",
    "            num_gold += 1\n",
    "        if prediction == 1 and gold == 1:\n",
    "            num_pos_correct += 1\n",
    "        num_total += 1\n",
    "\n",
    "    return num_correct * 100.0 / num_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_-kDQaaOGjgm",
    "outputId": "1e792839-a00b-4faa-8943-4a76f0450a35"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6920 train examples: 3610 positive, 3310 negative\n",
      "872 dev examples\n"
     ]
    }
   ],
   "source": [
    "# Load the data from the files\n",
    "train_exs = read_sentiment_examples('./data/train.txt')\n",
    "dev_exs = read_sentiment_examples('./data/dev.txt')\n",
    "n_pos = 0\n",
    "n_neg = 0\n",
    "for ex in train_exs:\n",
    "    if ex.label == 1:\n",
    "        n_pos += 1\n",
    "    else:\n",
    "        n_neg += 1\n",
    "print(\"%d train examples: %d positive, %d negative\" % (len(train_exs), n_pos, n_neg))\n",
    "print(\"%d dev examples\" % len(dev_exs))\n",
    "\n",
    "\n",
    "# Evaluate on train and dev dataset\n",
    "def eval_train_dev(model):\n",
    "    train_acc = evaluate(model, train_exs)\n",
    "    eval_acc = evaluate(model, dev_exs)\n",
    "    return [train_acc, eval_acc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "P06OvAGgCxOB",
    "outputId": "dc5edce6-5e13-414d-f5e0-dd93015864b4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[99.98554913294798, 66.05504587155963]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate logistic regression with unigram features\n",
    "unigram_model = train_model('unigram', 'LogisticRegression', train_exs)\n",
    "eval_train_dev(unigram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Cb5wnlJtFwHj",
    "outputId": "206012ae-1d25-4f9e-ccc4-326ed617e456"
   },
   "outputs": [],
   "source": [
    "# Evaluate logistic regression with bigram features\n",
    "bigram_model = train_model('bigram', 'LogisticRegression', train_exs)\n",
    "eval_train_dev(bigram_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
